{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* PEFT technique: LoRA on quantized model\n",
    "* Model: Gemma-7B\n",
    "* Evaluation approach: accuracy_score from sklearn on sentiment keyword matching\n",
    "* Fine-tuning dataset: [Twitter Sentiment Analysis](https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-30T17:26:58.760665Z",
     "iopub.status.busy": "2024-03-30T17:26:58.759994Z",
     "iopub.status.idle": "2024-03-30T17:33:11.876027Z",
     "shell.execute_reply": "2024-03-30T17:33:11.874639Z",
     "shell.execute_reply.started": "2024-03-30T17:26:58.760627Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -Uqqq pip --progress-bar off\n",
    "!pip install -qqq langchain==0.0.228 --progress-bar off\n",
    "!pip install -qqq chromadb==0.3.26 --progress-bar off\n",
    "!pip install -qqq sentence-transformers==2.2.2 --progress-bar off\n",
    "!pip install -qqq auto-gptq==0.2.2 --progress-bar off\n",
    "!pip install -qqq einops==0.6.1 --progress-bar off\n",
    "!pip install -qqq unstructured==0.8.0 --progress-bar off\n",
    "!pip install -qqq torch==2.0.1 --progress-bar off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T17:33:11.878284Z",
     "iopub.status.busy": "2024-03-30T17:33:11.877975Z",
     "iopub.status.idle": "2024-03-30T17:35:55.121280Z",
     "shell.execute_reply": "2024-03-30T17:35:55.119912Z",
     "shell.execute_reply.started": "2024-03-30T17:33:11.878257Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets==2.16.0\n",
    "!pip install -qqq bitsandbytes\n",
    "!pip install -qqq  git+https://github.com/lyhue1991/torchkeras \n",
    "!pip install -qqq git+https://github.com/lvwerra/trl.git\n",
    "!pip install -q transformers==\"4.38.2\"\n",
    "!pip install -Uqqq git+https://github.com/huggingface/peft  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T17:35:55.123653Z",
     "iopub.status.busy": "2024-03-30T17:35:55.123256Z",
     "iopub.status.idle": "2024-03-30T17:36:11.056883Z",
     "shell.execute_reply": "2024-03-30T17:36:11.056071Z",
     "shell.execute_reply.started": "2024-03-30T17:35:55.123619Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 17:36:02.668307: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-30 17:36:02.668424: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-30 17:36:02.795926: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import accelerate \n",
    "import peft \n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
    "os.environ['XLA_USE_BF16'] = \"1\"\n",
    "os.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\n",
    "\n",
    "import torchkeras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T17:36:11.059503Z",
     "iopub.status.busy": "2024-03-30T17:36:11.058964Z",
     "iopub.status.idle": "2024-03-30T17:36:11.085103Z",
     "shell.execute_reply": "2024-03-30T17:36:11.084270Z",
     "shell.execute_reply.started": "2024-03-30T17:36:11.059477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571a4182c6984ae1940b8a6ede9cd2ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T17:44:41.897356Z",
     "iopub.status.busy": "2024-03-30T17:44:41.896957Z",
     "iopub.status.idle": "2024-03-30T17:48:03.730112Z",
     "shell.execute_reply": "2024-03-30T17:48:03.729015Z",
     "shell.execute_reply.started": "2024-03-30T17:44:41.897325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c18421357c41a3898654fb1368bbed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c8168649684fdba9622103c7329d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806a656da98b46d0a377fe404a56708c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a37296ab194665911ab53435425dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/555 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712c443114f941d09b0c55308840a45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026689099efb45b29c6a84db014e7511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9bc402add9469bb041795387973686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9697f413ee4b49028a14f9d88d291184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca65a4b9ba649509cd370c83ac02a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101d85f9722d46c3a008d5f55cc5cd93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c69fda294434f45bb544fb80879cf72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c0db9c52c7e496aa3f51c351c841176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85573ec0f9f74964896ea051d6ebc83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write me about roronoa zoro from one piece\n",
      "\n",
      "Answer:\n",
      "\n",
      "Step 1/\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "        \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "max_seq_length = 2048\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\", max_seq_length=max_seq_length)\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", device_map=\"auto\", quantization_config=bnb_config)\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "input_text = \"Write me about roronoa zoro from one piece\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T17:48:03.732822Z",
     "iopub.status.busy": "2024-03-30T17:48:03.732212Z",
     "iopub.status.idle": "2024-03-30T17:48:04.040980Z",
     "shell.execute_reply": "2024-03-30T17:48:04.040031Z",
     "shell.execute_reply.started": "2024-03-30T17:48:03.732786Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('/kaggle/input/twitter-entity-sentiment-analysis/twitter_training.csv')\n",
    "df_val=pd.read_csv('/kaggle/input/twitter-entity-sentiment-analysis/twitter_validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T17:48:04.042576Z",
     "iopub.status.busy": "2024-03-30T17:48:04.042217Z",
     "iopub.status.idle": "2024-03-30T17:48:04.050730Z",
     "shell.execute_reply": "2024-03-30T17:48:04.049696Z",
     "shell.execute_reply.started": "2024-03-30T17:48:04.042531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['2401', 'Borderlands', 'Positive',\n",
       "       'im getting on borderlands and i will murder you all ,'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T17:48:04.053407Z",
     "iopub.status.busy": "2024-03-30T17:48:04.052868Z",
     "iopub.status.idle": "2024-03-30T17:48:04.061022Z",
     "shell.execute_reply": "2024-03-30T17:48:04.060166Z",
     "shell.execute_reply.started": "2024-03-30T17:48:04.053373Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['3364', 'Facebook', 'Irrelevant',\n",
       "       'I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tomâ€™s great auntie as â€˜Hayley canâ€™t get out of bedâ€™ and told to his grandma, who now thinks Iâ€™m a lazy, terrible person ðŸ¤£'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T17:48:04.062651Z",
     "iopub.status.busy": "2024-03-30T17:48:04.062266Z",
     "iopub.status.idle": "2024-03-30T17:48:04.083183Z",
     "shell.execute_reply": "2024-03-30T17:48:04.082344Z",
     "shell.execute_reply.started": "2024-03-30T17:48:04.062624Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_train.rename(columns={'Positive': 'sentiment','im getting on borderlands and i will murder you all ,':'text' }, inplace=True)\n",
    "df_val.rename(columns={'Irrelevant': 'sentiment','I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tomâ€™s great auntie as â€˜Hayley canâ€™t get out of bedâ€™ and told to his grandma, who now thinks Iâ€™m a lazy, terrible person ðŸ¤£':'text' }, inplace=True)\n",
    "df_val=df_val.drop(['3364','Facebook'],axis=1)\n",
    "df_train=df_train.drop(['Borderlands','2401'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T18:06:42.929931Z",
     "iopub.status.busy": "2024-03-30T18:06:42.929081Z",
     "iopub.status.idle": "2024-03-30T18:06:45.648918Z",
     "shell.execute_reply": "2024-03-30T18:06:45.647647Z",
     "shell.execute_reply.started": "2024-03-30T18:06:42.929896Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train = df_train[df_train['sentiment'] != 'Irrelevant']\n",
    "df_val = df_val[df_val['sentiment'] != 'Irrelevant']\n",
    "\n",
    "df_train['sentiment'] = df_train['sentiment'].str.lower()\n",
    "df_val['sentiment'] = df_val['sentiment'].str.lower()\n",
    "\n",
    "X_train = list()\n",
    "X_test = list()\n",
    "columns_to_check = ['positive', 'negative', 'neutral']\n",
    "for sentiment in columns_to_check:\n",
    "    train, test = train_test_split(df_train[df_train.sentiment == sentiment], \n",
    "                                   train_size=200,\n",
    "                                   test_size=50, \n",
    "                                   random_state=42)\n",
    "    X_train.append(train)\n",
    "    X_test.append(test)\n",
    "\n",
    "X_train = pd.concat(X_train).sample(frac=1, random_state=10)\n",
    "X_test = pd.concat(X_test)\n",
    "\n",
    "eval_idx = [idx for idx in df_train.index if idx not in list(train.index) + list(test.index)]\n",
    "X_eval = df_train[df_train.index.isin(eval_idx)]\n",
    "X_eval = (X_eval\n",
    "          .groupby('sentiment', group_keys=False)\n",
    "          .apply(lambda x: x.sample(n=50, random_state=10, replace=True)))\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"generate_prompt\n",
    "            Analyze the sentiment of the comment enclosed in square brackets, \n",
    "            determine if it is vulgar, hate, religious, threat, troll, insult and neutral. It can have multiple labels among them. return the answer as \n",
    "            the corresponding sentiment label vulgar, hate, religious, threat, troll, insult or neutral. \n",
    "\n",
    "            [{data_point[\"text\"]}] = {data_point[\"sentiment\"]}\n",
    "            \"\"\".strip() + EOS_TOKEN\n",
    "\n",
    "def generate_test_prompt(data_point):\n",
    "    return f\"\"\"generate_prompt\n",
    "            Analyze the sentiment of the comment enclosed in square brackets, \n",
    "            determine if it is positive, negative or neutral. It can have multiple labels among them. return the answer as \n",
    "            the corresponding sentiment label positive, negative or neutral. \n",
    "\n",
    "\n",
    "            [{data_point[\"text\"]}] = \n",
    "\n",
    "            \"\"\".strip()\n",
    "\n",
    "X_train = pd.DataFrame(X_train.apply(generate_prompt, axis=1), \n",
    "                       columns=[\"text\"])\n",
    "X_eval = pd.DataFrame(X_eval.apply(generate_prompt, axis=1), \n",
    "                      columns=[\"text\"])\n",
    "\n",
    "y_true = X_test.sentiment\n",
    "X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T18:06:56.223850Z",
     "iopub.status.busy": "2024-03-30T18:06:56.222977Z",
     "iopub.status.idle": "2024-03-30T18:06:56.246207Z",
     "shell.execute_reply": "2024-03-30T18:06:56.245245Z",
     "shell.execute_reply.started": "2024-03-30T18:06:56.223817Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_data = Dataset.from_pandas(X_train)\n",
    "eval_data = Dataset.from_pandas(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T18:06:56.496790Z",
     "iopub.status.busy": "2024-03-30T18:06:56.496072Z",
     "iopub.status.idle": "2024-03-30T18:06:56.504092Z",
     "shell.execute_reply": "2024-03-30T18:06:56.503089Z",
     "shell.execute_reply.started": "2024-03-30T18:06:56.496760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'generate_prompt\\n            Analyze the sentiment of the comment enclosed in square brackets, \\n            determine if it is vulgar, hate, religious, threat, troll, insult and neutral. It can have multiple labels among them. return the answer as \\n            the corresponding sentiment label vulgar, hate, religious, threat, troll, insult or neutral. \\n\\n            [The Johnson & General Johnson Halts Talc Ltd Baby Powder Horse Sales Management wb. md / 2ylv9sV] = neutral<eos>'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['text'][41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T18:06:56.793494Z",
     "iopub.status.busy": "2024-03-30T18:06:56.793189Z",
     "iopub.status.idle": "2024-03-30T18:06:56.807520Z",
     "shell.execute_reply": "2024-03-30T18:06:56.806591Z",
     "shell.execute_reply.started": "2024-03-30T18:06:56.793469Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    labels = ['positive', 'neutral', 'negative']\n",
    "    mapping = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
    "    \n",
    "    def map_func(x):\n",
    "        return mapping.get(x, 1)\n",
    "    \n",
    "    y_true_mapped = np.vectorize(map_func)(y_true)\n",
    "    y_pred_mapped = np.vectorize(map_func)(y_pred)\n",
    "    \n",
    "    accuracy = accuracy_score(y_true=y_true_mapped, y_pred=y_pred_mapped)\n",
    "    print(f'Overall Accuracy: {accuracy:.3f}')\n",
    "    \n",
    "    unique_labels = set(y_true_mapped)\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        label_indices = [i for i, y in enumerate(y_true_mapped) if y == label]\n",
    "        label_y_true = [y_true_mapped[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred_mapped[i] for i in label_indices]\n",
    "        label_accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        print(f'Accuracy for label {labels[label]}: {label_accuracy:.3f}')\n",
    "        \n",
    "    class_report = classification_report(y_true=y_true_mapped, y_pred=y_pred_mapped, target_names=labels)\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_true=y_true_mapped, y_pred=y_pred_mapped, labels=[0, 1, 2])\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T18:06:57.032436Z",
     "iopub.status.busy": "2024-03-30T18:06:57.031832Z",
     "iopub.status.idle": "2024-03-30T18:06:57.040687Z",
     "shell.execute_reply": "2024-03-30T18:06:57.039694Z",
     "shell.execute_reply.started": "2024-03-30T18:06:57.032408Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(X_test, model, tokenizer):\n",
    "    y_pred = []\n",
    "    for i in tqdm(range(len(X_test))):\n",
    "        prompt = X_test.iloc[i][\"text\"]\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model.generate(**input_ids, max_new_tokens=1, temperature=0.0)\n",
    "        result = tokenizer.decode(outputs[0])\n",
    "        answer = result.split(\"=\")[-1].lower()\n",
    "        if \"positive\" in answer:\n",
    "            y_pred.append(\"positive\")\n",
    "        elif \"negative\" in answer:\n",
    "            y_pred.append(\"negative\")\n",
    "        else:\n",
    "            y_pred.append(\"neutral\")\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T18:06:57.215662Z",
     "iopub.status.busy": "2024-03-30T18:06:57.215003Z",
     "iopub.status.idle": "2024-03-30T18:09:48.751922Z",
     "shell.execute_reply": "2024-03-30T18:09:48.750761Z",
     "shell.execute_reply.started": "2024-03-30T18:06:57.215638Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/150 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [02:51<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "y_pred = predict(X_test, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T18:09:48.754373Z",
     "iopub.status.busy": "2024-03-30T18:09:48.754003Z",
     "iopub.status.idle": "2024-03-30T18:09:48.780161Z",
     "shell.execute_reply": "2024-03-30T18:09:48.778107Z",
     "shell.execute_reply.started": "2024-03-30T18:09:48.754338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.587\n",
      "Accuracy for label positive: 0.840\n",
      "Accuracy for label neutral: 0.020\n",
      "Accuracy for label negative: 0.900\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.64      0.84      0.72        50\n",
      "     neutral       1.00      0.02      0.04        50\n",
      "    negative       0.54      0.90      0.68        50\n",
      "\n",
      "    accuracy                           0.59       150\n",
      "   macro avg       0.73      0.59      0.48       150\n",
      "weighted avg       0.73      0.59      0.48       150\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  0  8]\n",
      " [19  1 30]\n",
      " [ 5  0 45]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base model achieves an accuracy of 58.7%. Additionally, it performs poorly on negative texts but excels with neutral ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T18:09:48.782295Z",
     "iopub.status.busy": "2024-03-30T18:09:48.781878Z",
     "iopub.status.idle": "2024-03-30T18:09:48.810430Z",
     "shell.execute_reply": "2024-03-30T18:09:48.809480Z",
     "shell.execute_reply.started": "2024-03-30T18:09:48.782258Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import get_peft_config, get_peft_model, TaskType\n",
    "\n",
    "# Enable gradient checkpointing support in the model\n",
    "model.supports_gradient_checkpointing = True  #\n",
    "\n",
    "# Enable gradient checkpointing for more memory-efficient training\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Enable requiring gradients for model inputs\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# Disable cache usage in the model configuration to silence warnings \n",
    "model.config.use_cache = False  #  Re-enable for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T18:09:48.814266Z",
     "iopub.status.busy": "2024-03-30T18:09:48.813432Z",
     "iopub.status.idle": "2024-03-30T18:09:48.839628Z",
     "shell.execute_reply": "2024-03-30T18:09:48.838324Z",
     "shell.execute_reply.started": "2024-03-30T18:09:48.814221Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['base_layer']\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    \"\"\"\n",
    "    Find all fully connected layers and add low-rank adapters to each one.\n",
    "    \"\"\"\n",
    "    cls = bnb.nn.Linear4bit\n",
    "\n",
    "    lora_module_names = set()\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:\n",
    "        lora_module_names.remove('lm_head')\n",
    "\n",
    "    return list(lora_module_names)\n",
    "\n",
    "lora_modules = find_all_linear_names(model)\n",
    "print(lora_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T18:09:48.841424Z",
     "iopub.status.busy": "2024-03-30T18:09:48.841080Z",
     "iopub.status.idle": "2024-03-30T18:09:50.225599Z",
     "shell.execute_reply": "2024-03-30T18:09:50.224437Z",
     "shell.execute_reply.started": "2024-03-30T18:09:48.841391Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=lora_modules\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "peft_model.is_parallelizable = True\n",
    "peft_model.model_parallel = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T18:09:50.228847Z",
     "iopub.status.busy": "2024-03-30T18:09:50.226979Z",
     "iopub.status.idle": "2024-03-30T18:09:50.264051Z",
     "shell.execute_reply": "2024-03-30T18:09:50.262993Z",
     "shell.execute_reply.started": "2024-03-30T18:09:50.228808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.lora_A.default.weight:\n",
      "shape =  [8, 3072] \t sum =  1.22685968875885\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.lora_B.default.weight:\n",
      "shape =  [4096, 8] \t sum =  0.0\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight:\n",
      "shape =  [8, 3072] \t sum =  -1.334961175918579\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight:\n",
      "shape =  [4096, 8] \t sum =  -0.005172288045287132\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.lora_A.default.weight:\n",
      "shape =  [8, 3072] \t sum =  0.9425515532493591\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.self_attn.k_proj.base_layer.lora_B.default.weight:\n",
      "shape =  [4096, 8] \t sum =  0.0\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight:\n",
      "shape =  [8, 3072] \t sum =  0.5233312845230103\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight:\n",
      "shape =  [4096, 8] \t sum =  -0.025704532861709595\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.lora_A.default.weight:\n",
      "shape =  [8, 3072] \t sum =  -1.265538215637207\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.lora_B.default.weight:\n",
      "shape =  [4096, 8] \t sum =  0.0\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight:\n",
      "shape =  [8, 3072] \t sum =  0.1325361132621765\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight:\n",
      "shape =  [4096, 8] \t sum =  -0.0974748432636261\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.lora_A.default.weight:\n",
      "shape =  [8, 4096] \t sum =  -0.08451735973358154\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.self_attn.o_proj.base_layer.lora_B.default.weight:\n",
      "shape =  [3072, 8] \t sum =  0.0\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight:\n",
      "shape =  [8, 4096] \t sum =  1.963411808013916\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight:\n",
      "shape =  [3072, 8] \t sum =  0.03276874125003815\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.mlp.gate_proj.base_layer.lora_A.default.weight:\n",
      "shape =  [8, 3072] \t sum =  -0.4273686408996582\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.mlp.gate_proj.base_layer.lora_B.default.weight:\n",
      "shape =  [24576, 8] \t sum =  0.0\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight:\n",
      "shape =  [8, 3072] \t sum =  -1.2665225267410278\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight:\n",
      "shape =  [24576, 8] \t sum =  0.0793817937374115\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.mlp.up_proj.base_layer.lora_A.default.weight:\n",
      "shape =  [8, 3072] \t sum =  -5.348316192626953\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.mlp.up_proj.base_layer.lora_B.default.weight:\n",
      "shape =  [24576, 8] \t sum =  0.0\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight:\n",
      "shape =  [8, 3072] \t sum =  -1.1400129795074463\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight:\n",
      "shape =  [24576, 8] \t sum =  -0.005167707800865173\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.mlp.down_proj.base_layer.lora_A.default.weight:\n",
      "shape =  [8, 24576] \t sum =  -2.2288260459899902\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.mlp.down_proj.base_layer.lora_B.default.weight:\n",
      "shape =  [3072, 8] \t sum =  0.0\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight:\n",
      "shape =  [8, 24576] \t sum =  -1.0751280784606934\n",
      "\n",
      "\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight:\n",
      "shape =  [3072, 8] \t sum =  0.01632239855825901\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, para in peft_model.named_parameters():\n",
    "    # Break the loop if the name contains '.1.'\n",
    "    if '.1.' in name:\n",
    "        break\n",
    "    # Check if the parameter is related to LoRA (contains 'lora' in its name)\n",
    "    if 'lora' in name.lower():\n",
    "        # Print information about the parameter\n",
    "        print(name + ':')\n",
    "        print('shape = ', list(para.shape), '\\t', 'sum = ', para.sum().item())\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T18:11:46.526585Z",
     "iopub.status.busy": "2024-03-30T18:11:46.525868Z",
     "iopub.status.idle": "2024-03-30T18:11:46.537589Z",
     "shell.execute_reply": "2024-03-30T18:11:46.536337Z",
     "shell.execute_reply.started": "2024-03-30T18:11:46.526542Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./Gemma-7b\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=25,\n",
    "    logging_strategy=\"steps\",\n",
    "    max_steps=-1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=0,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    weight_decay=0.001,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=False,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=112,\n",
    "    eval_accumulation_steps=1,\n",
    "    lr_scheduler_type=\"cosine\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T18:11:47.540442Z",
     "iopub.status.busy": "2024-03-30T18:11:47.539668Z",
     "iopub.status.idle": "2024-03-30T18:11:50.002213Z",
     "shell.execute_reply": "2024-03-30T18:11:50.001220Z",
     "shell.execute_reply.started": "2024-03-30T18:11:47.540408Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851ebcddf8ce4eb2980ff99e8f7fe8bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ee1c7087564d0081e8adb5bd7f1a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    args=training_args,\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T18:11:50.005713Z",
     "iopub.status.busy": "2024-03-30T18:11:50.005409Z",
     "iopub.status.idle": "2024-03-30T19:36:01.393020Z",
     "shell.execute_reply": "2024-03-30T19:36:01.391909Z",
     "shell.execute_reply.started": "2024-03-30T18:11:50.005687Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [225/225 1:23:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.735000</td>\n",
       "      <td>4.438314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.288800</td>\n",
       "      <td>4.996609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "\n",
    "trainer.model.save_pretrained(\"trained-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T19:42:29.772060Z",
     "iopub.status.busy": "2024-03-30T19:42:29.771281Z",
     "iopub.status.idle": "2024-03-30T19:42:29.777192Z",
     "shell.execute_reply": "2024-03-30T19:42:29.776116Z",
     "shell.execute_reply.started": "2024-03-30T19:42:29.772028Z"
    }
   },
   "outputs": [],
   "source": [
    "model_trained = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T19:43:03.262538Z",
     "iopub.status.busy": "2024-03-30T19:43:03.261814Z",
     "iopub.status.idle": "2024-03-30T19:43:06.922770Z",
     "shell.execute_reply": "2024-03-30T19:43:06.921653Z",
     "shell.execute_reply.started": "2024-03-30T19:43:03.262505Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "ft_model = PeftModel.from_pretrained(model_trained, \"/kaggle/working/trained-model\",torch_dtype=torch.float16,is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T19:44:08.548478Z",
     "iopub.status.busy": "2024-03-30T19:44:08.547721Z",
     "iopub.status.idle": "2024-03-30T19:47:16.603965Z",
     "shell.execute_reply": "2024-03-30T19:47:16.601626Z",
     "shell.execute_reply.started": "2024-03-30T19:44:08.548442Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [03:08<00:00,  1.25s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "y_pred = predict(X_test, ft_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T19:47:16.606267Z",
     "iopub.status.busy": "2024-03-30T19:47:16.605904Z",
     "iopub.status.idle": "2024-03-30T19:47:16.628545Z",
     "shell.execute_reply": "2024-03-30T19:47:16.627437Z",
     "shell.execute_reply.started": "2024-03-30T19:47:16.606233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.727\n",
      "Accuracy for label positive: 0.860\n",
      "Accuracy for label neutral: 0.440\n",
      "Accuracy for label negative: 0.880\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.72      0.86      0.78        50\n",
      "     neutral       0.76      0.44      0.56        50\n",
      "    negative       0.72      0.88      0.79        50\n",
      "\n",
      "    accuracy                           0.73       150\n",
      "   macro avg       0.73      0.73      0.71       150\n",
      "weighted avg       0.73      0.73      0.71       150\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4  3]\n",
      " [14 22 14]\n",
      " [ 3  3 44]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the model's overall accuracy has significantly improved to 72.7%, its accuracy for neutral comments has decreased notably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1520310,
     "sourceId": 2510329,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
